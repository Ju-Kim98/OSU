{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, update 39.0%, dev 39.6%\n",
      "epoch 2, update 25.5%, dev 34.1%\n",
      "epoch 3, update 20.8%, dev 35.3%\n",
      "epoch 4, update 17.2%, dev 35.5%\n",
      "epoch 5, update 14.1%, dev 28.9%\n",
      "epoch 6, update 12.2%, dev 32.0%\n",
      "epoch 7, update 10.5%, dev 32.0%\n",
      "epoch 8, update 9.7%, dev 31.5%\n",
      "epoch 9, update 7.8%, dev 30.2%\n",
      "epoch 10, update 6.9%, dev 29.8%\n",
      "best dev err 28.9%, |w|=16744, time: 1.6 secs\n"
     ]
    }
   ],
   "source": [
    "# after add bias\n",
    "!python3 train.py train.txt dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svector import svector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = svector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svector(float, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['the']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svector(float, {'the': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a += a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svector(float, {'the': 2})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svector(float, {'the': 4})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svector(float, {'the': 4})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svector(float, {'the': -2})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['boy'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svector(float, {'boy': 1, 'the': 2})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot({'the': -1, 'girl': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'train.txt'\n",
    "dev_file = 'dev.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, update 39.0%, dev 39.6%\n",
      "epoch 2, update 25.5%, dev 34.1%\n",
      "epoch 3, update 20.8%, dev 35.3%\n",
      "epoch 4, update 17.2%, dev 35.5%\n",
      "epoch 5, update 14.1%, dev 28.9%\n",
      "epoch 6, update 12.2%, dev 32.0%\n",
      "epoch 7, update 10.5%, dev 32.0%\n",
      "epoch 8, update 9.7%, dev 31.5%\n",
      "epoch 9, update 7.8%, dev 30.2%\n",
      "epoch 10, update 6.9%, dev 29.8%\n",
      "best dev err 28.9%, |w|=16744, time: 2.8 secs\n"
     ]
    }
   ],
   "source": [
    "train(train_file, dev_file, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ave' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-ae1fafcae879>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_ave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_ave' is not defined"
     ]
    }
   ],
   "source": [
    "train_ave(train_file, dev_file, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-124-0da10127e7c2>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-124-0da10127e7c2>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    print(f\"{' '.join(words)}\")\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "    # Sort mistakes by confidence\n",
    "    mistakes.sort(key=lambda x: x[0])\n",
    "    # Get the top 5 negative examples that the model believes to be positive\n",
    "    false_positives = [mistake for mistake in mistakes if mistake[1] == -1][:5]\n",
    "    # Get the top 5 positive examples that the model believes to be negative\n",
    "    false_negatives = [mistake for mistake in mistakes if mistake[1] == 1][-5:]\n",
    "\n",
    "    print(\"\\nTop 5 Negative Examples that model believes to be Positive:\")\n",
    "    for margin, label, words in false_positives:\n",
    "      #  print(f\"{' '.join(words)}\")\n",
    "        print(' '.join(words))\n",
    "\n",
    "    print(\"\\nTop 5 Positive Examples that model believes to be Negative:\")\n",
    "    for margin, label, words in false_negatives:\n",
    "        print(f\"{' '.join(words)}\")\n",
    "\n",
    "    return model_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division # no need for python3, but just in case used w/ python2\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from svector import svector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_confidence(devfile, model):\n",
    "    results = []\n",
    "    for i, (label, words) in enumerate(read_from(devfile), 1):  # note 1...|D|\n",
    "        confidence = model.dot(make_vector(words))  # model's confidence in its prediction\n",
    "        results.append((label, confidence, ' '.join(words)))\n",
    "    return results\n",
    "\n",
    "def find_misclassified_negatives(devfile, model, n=5):\n",
    "    # Get the predictions with confidence scores\n",
    "    results = test_with_confidence(devfile, model)\n",
    "    # Sort by model's confidence in descending order\n",
    "    sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    # Filter out the negative examples that were misclassified as positive\n",
    "    misclassified_negatives = [(label, confidence, example) for label, confidence, example in sorted_results if label < 0]\n",
    "    # Return the top 'n' misclassified as positive examples\n",
    "    return misclassified_negatives[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-7f529e44048b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfind_misclassified_negatives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-79-2c8ad10dde99>\u001b[0m in \u001b[0;36mfind_misclassified_negatives\u001b[1;34m(devfile, model, n)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_misclassified_negatives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Get the predictions with confidence scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_with_confidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Sort by model's confidence in descending order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0msorted_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-2c8ad10dde99>\u001b[0m in \u001b[0;36mtest_with_confidence\u001b[1;34m(devfile, model)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# note 1...|D|\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mconfidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmake_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# model's confidence in its prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'dot'"
     ]
    }
   ],
   "source": [
    "find_misclassified_negatives(train_file, dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jujud\\Desktop\\AI534\\HW2\\hw2-data\\text_classification.py\", line 2, in <module>\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "ModuleNotFoundError: No module named 'sklearn'\n"
     ]
    }
   ],
   "source": [
    "!python3 text_classification.py train.txt dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jujud\\Desktop\\AI534\\HW2\\hw2-data\\text_classification.py\", line 2, in <module>\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "ModuleNotFoundError: No module named 'sklearn'\n"
     ]
    }
   ],
   "source": [
    "!python3 text_classification.py train.txt dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "less text_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jujud\\Desktop\\AI534\\HW2\\hw2-data\\text_classification.py\", line 2, in <module>\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "ModuleNotFoundError: No module named 'sklearn'\n"
     ]
    }
   ],
   "source": [
    "!python3 text_classification.py train.txt dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\jujud\\anaconda3\\envs\\py27\\lib\\site-packages (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\jujud\\anaconda3\\envs\\py27\\lib\\site-packages (from scikit-learn) (1.16.5)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\users\\jujud\\anaconda3\\envs\\py27\\lib\\site-packages (from scikit-learn) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.099999999999994\n",
      "time: 0.8 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jujud\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "!python text_classification.py train.txt dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.299999999999997\n",
      "('Training time', 0.5499999523162842)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'newline' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0e8f08d56d0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mget_predicted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-0e8f08d56d0d>\u001b[0m in \u001b[0;36mget_predicted\u001b[1;34m(test_file, vocabulary, model, scaler, output_file)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'newline' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming svector is a custom class or function you have that works like a sparse vector\n",
    "from svector import svector\n",
    "\n",
    "\n",
    "def read_data(textfile):\n",
    "    data = []\n",
    "    for line in open(textfile):\n",
    "        label, words = line.strip().split(\"\\t\")\n",
    "        data.append((1 if label == \"+\" else -1, words.split()))\n",
    "    return data\n",
    "\n",
    "\n",
    "def count_words(data):\n",
    "    word_counts = defaultdict(int)\n",
    "    for _, words in data:\n",
    "        for word in words:\n",
    "            word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def make_vector(words, word_counts, bias=True):\n",
    "    v = svector()\n",
    "    for word in words:\n",
    "        if word in word_counts:\n",
    "            v[word] += 1\n",
    "    if bias:\n",
    "        v['<bias>'] = 1\n",
    "    return v\n",
    "\n",
    "\n",
    "def prune_features(word_counts, min_count=2):\n",
    "    return {word: count for word, count in word_counts.items() if count >= min_count}\n",
    "\n",
    "\n",
    "def vector_to_nparray(vectors, vocabulary):\n",
    "    X = np.zeros((len(vectors), len(vocabulary)))\n",
    "    for i, vector in enumerate(vectors):\n",
    "        for word, value in vector.items():\n",
    "            if word in vocabulary:\n",
    "                X[i, vocabulary[word]] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "def logistic_regression(train_data, dev_data, word_counts):\n",
    "    pruned_word_counts = prune_features(word_counts)\n",
    "    vocabulary = {word: idx for idx, word in enumerate(pruned_word_counts.keys())}\n",
    "\n",
    "    X_train = vector_to_nparray([make_vector(words, pruned_word_counts) for _, words in train_data], vocabulary)\n",
    "    y_train = np.array([label for label, _ in train_data])\n",
    "    X_dev = vector_to_nparray([make_vector(words, pruned_word_counts) for _, words in dev_data], vocabulary)\n",
    "    y_dev = np.array([label for label, _ in dev_data])\n",
    "\n",
    "    scaler = MaxAbsScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_dev_scaled = scaler.transform(X_dev)\n",
    "\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_dev_pred = model.predict(X_dev_scaled)\n",
    "    accuracy = accuracy_score(y_dev, y_dev_pred) * 100\n",
    "    print(100 - accuracy)\n",
    "    print(\"Training time\",time.time() - start_time)\n",
    "\n",
    "    return vocabulary, model, scaler\n",
    "\n",
    "\n",
    "def get_predicted(test_file, vocabulary, model, scaler, output_file):\n",
    "    test_data = read_data(test_file)\n",
    "    X_test = vector_to_nparray([make_vector(words, vocabulary) for _, words in test_data], vocabulary)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='\\t')\n",
    "        for (label, words), pred in zip(test_data, predictions):\n",
    "            writer.writerow(['+' if pred == 1 else '-', ' '.join(words)])\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_file = 'train.txt'\n",
    "    dev_file = 'dev.txt'\n",
    "    test_file = 'test.txt'\n",
    "    output_file = 'test.txt.predicted'\n",
    "\n",
    "    train_data = read_data(train_file)\n",
    "    dev_data = read_data(dev_file)\n",
    "    word_counts = count_words(train_data)\n",
    "\n",
    "    vocabulary, model, scaler = logistic_regression(train_data, dev_data, word_counts)\n",
    "    get_predicted(test_file, vocabulary, model, scaler, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data(testfile, model, outputfile):\n",
    "    with open(outputfile, 'w') as output:\n",
    "        for _, words in read_from(testfile):\n",
    "            sent = make_vector(words)\n",
    "            sent['bias'] = 1\n",
    "            prediction = model.dot(sent)\n",
    "            label = '+' if prediction > 0 else '-'\n",
    "            print(label, ' '.join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.299999999999997\n",
      "('Training time', 0.6699998378753662)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'newline' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-b82d5489d70e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mget_predicted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-0e8f08d56d0d>\u001b[0m in \u001b[0;36mget_predicted\u001b[1;34m(test_file, vocabulary, model, scaler, output_file)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'newline' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_file = 'train.txt'\n",
    "    dev_file = 'dev.txt'\n",
    "    test_file = 'test.txt'\n",
    "    output_file = 'test.txt.predicted'\n",
    "\n",
    "    train_data = read_data(train_file)\n",
    "    dev_data = read_data(dev_file)\n",
    "    word_counts = count_words(train_data)\n",
    "\n",
    "    vocabulary, model, scaler = logistic_regression(train_data, dev_data, word_counts)\n",
    "    get_predicted(test_file, vocabulary, model, scaler, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.099999999999994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_from(textfile):\n",
    "    word, sign = [], []\n",
    "    for line in open(textfile):\n",
    "        label, words = line.strip().split(\"\\t\")\n",
    "        sign.append(1 if label == \"+\" else -1)\n",
    "        word.append(' '.join(words.split()))\n",
    "    return word, sign\n",
    "\n",
    "def train_and_predict():\n",
    "    trainfile = 'train.txt'\n",
    "    devfile = 'dev.txt'\n",
    "    testfile = 'test.txt'\n",
    "\n",
    "    word_train, sign_train = read_from(trainfile)\n",
    "    word_dev, sign_dev = read_from(devfile)\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=2, token_pattern=r\"\\b\\w{2,}\\b\")\n",
    "    word_train = vectorizer.fit_transform(word_train)\n",
    "    word_dev_Bi = vectorizer.transform(word_dev)\n",
    "\n",
    "    # Experiment with hyperparameters\n",
    "    model = LogisticRegression(max_iter=1000, C=1.0)\n",
    "\n",
    "    model.fit(word_train, sign_train)\n",
    "    sign_pred = model.predict(word_dev_Bi)\n",
    "    accuracy = accuracy_score(sign_dev, sign_pred) * 100\n",
    "    print(100 - accuracy)\n",
    "\n",
    "    #print(f\"Accuracy on the development set: {accuracy:.2f}%\")\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    word_test, _ = read_from(testfile)\n",
    "    word_test_Bi = vectorizer.transform(word_test)\n",
    "    test_predictions = model.predict(word_test_Bi)\n",
    "\n",
    "    # Save the predictions to a file\n",
    "    with open('test.txt.predicted', 'w') as f:\n",
    "        for prediction in test_predictions:\n",
    "            label = '+' if prediction == 1 else '-'\n",
    "            f.write(label + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.txt\", sep='\\t', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?</td>\n",
       "      <td>a seriocomic debut of extravagant promise by g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?</td>\n",
       "      <td>while you have to admit it 's semi amusing to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?</td>\n",
       "      <td>a boldly stroked , luridly coloured , uni dime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>?</td>\n",
       "      <td>an enormously entertaining movie , like nothin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>?</td>\n",
       "      <td>in his latest effort , storytelling , solondz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>?</td>\n",
       "      <td>missteps take what was otherwise a fascinating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>?</td>\n",
       "      <td>its use of the thriller form to examine the la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>?</td>\n",
       "      <td>the master of disguise is awful it 's pauly sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>?</td>\n",
       "      <td>when you find yourself rooting for the monster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>?</td>\n",
       "      <td>what 's most memorable about circuit is that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>?</td>\n",
       "      <td>as saccharine as it is disposable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>?</td>\n",
       "      <td>cherish would 've worked a lot better had it b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>?</td>\n",
       "      <td>with ichi the killer , takashi miike , japan '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>?</td>\n",
       "      <td>the filmmakers are playing to the big boys in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>?</td>\n",
       "      <td>this latest installment of the horror film fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>?</td>\n",
       "      <td>smith finds amusing juxtapositions that justif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>?</td>\n",
       "      <td>those eternally devoted to the insanity of bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>?</td>\n",
       "      <td>a bland , obnoxious 88 minute infomercial for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>?</td>\n",
       "      <td>this is a movie that refreshes the mind and sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>?</td>\n",
       "      <td>eckstraordinarily lame and severely boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>?</td>\n",
       "      <td>both exuberantly romantic and serenely melanch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>?</td>\n",
       "      <td>the movie is n't painfully bad , something to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>?</td>\n",
       "      <td>rather than real figures , elling and kjell bj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>?</td>\n",
       "      <td>though it was made with careful attention to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>?</td>\n",
       "      <td>all the queen 's men is a throwback war movie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>?</td>\n",
       "      <td>an adorably whimsical comedy that deserves mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>?</td>\n",
       "      <td>this submarine drama earns the right to be fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>?</td>\n",
       "      <td>a compelling story of musical passion against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>?</td>\n",
       "      <td>it has the right approach and the right openin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>?</td>\n",
       "      <td>according to wendigo , ` nature ' loves the me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>?</td>\n",
       "      <td>it 's a shame that the storyline and its under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>?</td>\n",
       "      <td>if anything , see it for karen black , who cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>?</td>\n",
       "      <td>an intimate contemplation of two marvelously m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>?</td>\n",
       "      <td>the only type of lives this glossy comedy dram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>?</td>\n",
       "      <td>proves a lovely trifle that , unfortunately , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>?</td>\n",
       "      <td>enormously enjoyable , high adrenaline documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>?</td>\n",
       "      <td>oddly , the film is n't nearly as downbeat as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>?</td>\n",
       "      <td>an unsophisticated sci fi drama that takes its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>?</td>\n",
       "      <td>once the expectation of laughter has been quas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>?</td>\n",
       "      <td>one of the worst movies of the year watching i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>?</td>\n",
       "      <td>manages to delight without much of a story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>?</td>\n",
       "      <td>although it starts off so bad that you feel li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>?</td>\n",
       "      <td>maryam is a small film , but it offers large r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>?</td>\n",
       "      <td>poignant if familiar story of a young person s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>?</td>\n",
       "      <td>a ho hum affair , always watchable yet hardly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>?</td>\n",
       "      <td>the gags that fly at such a furiously funny pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>?</td>\n",
       "      <td>it 's not a bad plot but , unfortunately , the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>?</td>\n",
       "      <td>we get the comedy we settle for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>?</td>\n",
       "      <td>enchanted with low life tragedy and liberally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>?</td>\n",
       "      <td>ice age wo n't drop your jaw , but it will war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>?</td>\n",
       "      <td>like a precious and finely cut diamond , magni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>?</td>\n",
       "      <td>the film has a laundry list of minor shortcomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>?</td>\n",
       "      <td>whether it 's the worst movie of 2002 , i ca n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>?</td>\n",
       "      <td>exploring value choices is a worthwhile topic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>?</td>\n",
       "      <td>talkiness is n't necessarily bad , but the dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>?</td>\n",
       "      <td>the best thing i can say about this film is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>?</td>\n",
       "      <td>wonder of wonders a teen movie with a humanist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>?</td>\n",
       "      <td>the wild thornberrys movie is pleasant enough ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>?</td>\n",
       "      <td>as a thoughtful and unflinching examination of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>?</td>\n",
       "      <td>i 'd rather watch a rerun of the powerpuff girls</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                                                  1\n",
       "0    ?  a seriocomic debut of extravagant promise by g...\n",
       "1    ?  while you have to admit it 's semi amusing to ...\n",
       "2    ?  a boldly stroked , luridly coloured , uni dime...\n",
       "3    ?  an enormously entertaining movie , like nothin...\n",
       "4    ?  in his latest effort , storytelling , solondz ...\n",
       "5    ?  missteps take what was otherwise a fascinating...\n",
       "6    ?  its use of the thriller form to examine the la...\n",
       "7    ?  the master of disguise is awful it 's pauly sh...\n",
       "8    ?  when you find yourself rooting for the monster...\n",
       "9    ?  what 's most memorable about circuit is that i...\n",
       "10   ?                  as saccharine as it is disposable\n",
       "11   ?  cherish would 've worked a lot better had it b...\n",
       "12   ?  with ichi the killer , takashi miike , japan '...\n",
       "13   ?  the filmmakers are playing to the big boys in ...\n",
       "14   ?  this latest installment of the horror film fra...\n",
       "15   ?  smith finds amusing juxtapositions that justif...\n",
       "16   ?  those eternally devoted to the insanity of bla...\n",
       "17   ?  a bland , obnoxious 88 minute infomercial for ...\n",
       "18   ?  this is a movie that refreshes the mind and sp...\n",
       "19   ?         eckstraordinarily lame and severely boring\n",
       "20   ?  both exuberantly romantic and serenely melanch...\n",
       "21   ?  the movie is n't painfully bad , something to ...\n",
       "22   ?  rather than real figures , elling and kjell bj...\n",
       "23   ?  though it was made with careful attention to d...\n",
       "24   ?  all the queen 's men is a throwback war movie ...\n",
       "25   ?  an adorably whimsical comedy that deserves mor...\n",
       "26   ?  this submarine drama earns the right to be fav...\n",
       "27   ?  a compelling story of musical passion against ...\n",
       "28   ?  it has the right approach and the right openin...\n",
       "29   ?  according to wendigo , ` nature ' loves the me...\n",
       "..  ..                                                ...\n",
       "970  ?  it 's a shame that the storyline and its under...\n",
       "971  ?  if anything , see it for karen black , who cam...\n",
       "972  ?  an intimate contemplation of two marvelously m...\n",
       "973  ?  the only type of lives this glossy comedy dram...\n",
       "974  ?  proves a lovely trifle that , unfortunately , ...\n",
       "975  ?  enormously enjoyable , high adrenaline documen...\n",
       "976  ?  oddly , the film is n't nearly as downbeat as ...\n",
       "977  ?  an unsophisticated sci fi drama that takes its...\n",
       "978  ?  once the expectation of laughter has been quas...\n",
       "979  ?  one of the worst movies of the year watching i...\n",
       "980  ?         manages to delight without much of a story\n",
       "981  ?  although it starts off so bad that you feel li...\n",
       "982  ?  maryam is a small film , but it offers large r...\n",
       "983  ?  poignant if familiar story of a young person s...\n",
       "984  ?  a ho hum affair , always watchable yet hardly ...\n",
       "985  ?  the gags that fly at such a furiously funny pa...\n",
       "986  ?  it 's not a bad plot but , unfortunately , the...\n",
       "987  ?                    we get the comedy we settle for\n",
       "988  ?  enchanted with low life tragedy and liberally ...\n",
       "989  ?  ice age wo n't drop your jaw , but it will war...\n",
       "990  ?  like a precious and finely cut diamond , magni...\n",
       "991  ?  the film has a laundry list of minor shortcomi...\n",
       "992  ?  whether it 's the worst movie of 2002 , i ca n...\n",
       "993  ?  exploring value choices is a worthwhile topic ...\n",
       "994  ?  talkiness is n't necessarily bad , but the dia...\n",
       "995  ?  the best thing i can say about this film is th...\n",
       "996  ?  wonder of wonders a teen movie with a humanist...\n",
       "997  ?  the wild thornberrys movie is pleasant enough ...\n",
       "998  ?  as a thoughtful and unflinching examination of...\n",
       "999  ?   i 'd rather watch a rerun of the powerpuff girls\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = pd.read_csv(\"test.txt.predicted\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    +\n",
       "1    -\n",
       "2    -\n",
       "3    +\n",
       "4    -\n",
       "5    +\n",
       "6    +\n",
       "7    -\n",
       "8    -\n",
       "9    +\n",
       "10   -\n",
       "11   -\n",
       "12   +\n",
       "13   -\n",
       "14   +\n",
       "15   -\n",
       "16   -\n",
       "17   -\n",
       "18   +\n",
       "19   -\n",
       "20   +\n",
       "21   -\n",
       "22   +\n",
       "23   +\n",
       "24   -\n",
       "25   -\n",
       "26   +\n",
       "27   +\n",
       "28   -\n",
       "29   -\n",
       "..  ..\n",
       "970  -\n",
       "971  -\n",
       "972  +\n",
       "973  -\n",
       "974  -\n",
       "975  +\n",
       "976  +\n",
       "977  -\n",
       "978  -\n",
       "979  -\n",
       "980  +\n",
       "981  -\n",
       "982  +\n",
       "983  +\n",
       "984  +\n",
       "985  -\n",
       "986  -\n",
       "987  -\n",
       "988  +\n",
       "989  +\n",
       "990  +\n",
       "991  +\n",
       "992  -\n",
       "993  +\n",
       "994  -\n",
       "995  -\n",
       "996  +\n",
       "997  +\n",
       "998  +\n",
       "999  -\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop([0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a seriocomic debut of extravagant promise by g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>while you have to admit it 's semi amusing to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a boldly stroked , luridly coloured , uni dime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an enormously entertaining movie , like nothin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in his latest effort , storytelling , solondz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>missteps take what was otherwise a fascinating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>its use of the thriller form to examine the la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the master of disguise is awful it 's pauly sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>when you find yourself rooting for the monster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what 's most memorable about circuit is that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as saccharine as it is disposable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cherish would 've worked a lot better had it b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>with ichi the killer , takashi miike , japan '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the filmmakers are playing to the big boys in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>this latest installment of the horror film fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>smith finds amusing juxtapositions that justif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>those eternally devoted to the insanity of bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>a bland , obnoxious 88 minute infomercial for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>this is a movie that refreshes the mind and sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>eckstraordinarily lame and severely boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>both exuberantly romantic and serenely melanch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>the movie is n't painfully bad , something to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rather than real figures , elling and kjell bj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>though it was made with careful attention to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>all the queen 's men is a throwback war movie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>an adorably whimsical comedy that deserves mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>this submarine drama earns the right to be fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>a compelling story of musical passion against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>it has the right approach and the right openin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>according to wendigo , ` nature ' loves the me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>it 's a shame that the storyline and its under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>if anything , see it for karen black , who cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>an intimate contemplation of two marvelously m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>the only type of lives this glossy comedy dram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>proves a lovely trifle that , unfortunately , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>enormously enjoyable , high adrenaline documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>oddly , the film is n't nearly as downbeat as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>an unsophisticated sci fi drama that takes its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>once the expectation of laughter has been quas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>one of the worst movies of the year watching i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>manages to delight without much of a story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>although it starts off so bad that you feel li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>maryam is a small film , but it offers large r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>poignant if familiar story of a young person s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>a ho hum affair , always watchable yet hardly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>the gags that fly at such a furiously funny pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>it 's not a bad plot but , unfortunately , the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>we get the comedy we settle for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>enchanted with low life tragedy and liberally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>ice age wo n't drop your jaw , but it will war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>like a precious and finely cut diamond , magni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>the film has a laundry list of minor shortcomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>whether it 's the worst movie of 2002 , i ca n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>exploring value choices is a worthwhile topic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>talkiness is n't necessarily bad , but the dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>the best thing i can say about this film is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>wonder of wonders a teen movie with a humanist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>the wild thornberrys movie is pleasant enough ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>as a thoughtful and unflinching examination of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>i 'd rather watch a rerun of the powerpuff girls</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     1\n",
       "0    a seriocomic debut of extravagant promise by g...\n",
       "1    while you have to admit it 's semi amusing to ...\n",
       "2    a boldly stroked , luridly coloured , uni dime...\n",
       "3    an enormously entertaining movie , like nothin...\n",
       "4    in his latest effort , storytelling , solondz ...\n",
       "5    missteps take what was otherwise a fascinating...\n",
       "6    its use of the thriller form to examine the la...\n",
       "7    the master of disguise is awful it 's pauly sh...\n",
       "8    when you find yourself rooting for the monster...\n",
       "9    what 's most memorable about circuit is that i...\n",
       "10                   as saccharine as it is disposable\n",
       "11   cherish would 've worked a lot better had it b...\n",
       "12   with ichi the killer , takashi miike , japan '...\n",
       "13   the filmmakers are playing to the big boys in ...\n",
       "14   this latest installment of the horror film fra...\n",
       "15   smith finds amusing juxtapositions that justif...\n",
       "16   those eternally devoted to the insanity of bla...\n",
       "17   a bland , obnoxious 88 minute infomercial for ...\n",
       "18   this is a movie that refreshes the mind and sp...\n",
       "19          eckstraordinarily lame and severely boring\n",
       "20   both exuberantly romantic and serenely melanch...\n",
       "21   the movie is n't painfully bad , something to ...\n",
       "22   rather than real figures , elling and kjell bj...\n",
       "23   though it was made with careful attention to d...\n",
       "24   all the queen 's men is a throwback war movie ...\n",
       "25   an adorably whimsical comedy that deserves mor...\n",
       "26   this submarine drama earns the right to be fav...\n",
       "27   a compelling story of musical passion against ...\n",
       "28   it has the right approach and the right openin...\n",
       "29   according to wendigo , ` nature ' loves the me...\n",
       "..                                                 ...\n",
       "970  it 's a shame that the storyline and its under...\n",
       "971  if anything , see it for karen black , who cam...\n",
       "972  an intimate contemplation of two marvelously m...\n",
       "973  the only type of lives this glossy comedy dram...\n",
       "974  proves a lovely trifle that , unfortunately , ...\n",
       "975  enormously enjoyable , high adrenaline documen...\n",
       "976  oddly , the film is n't nearly as downbeat as ...\n",
       "977  an unsophisticated sci fi drama that takes its...\n",
       "978  once the expectation of laughter has been quas...\n",
       "979  one of the worst movies of the year watching i...\n",
       "980         manages to delight without much of a story\n",
       "981  although it starts off so bad that you feel li...\n",
       "982  maryam is a small film , but it offers large r...\n",
       "983  poignant if familiar story of a young person s...\n",
       "984  a ho hum affair , always watchable yet hardly ...\n",
       "985  the gags that fly at such a furiously funny pa...\n",
       "986  it 's not a bad plot but , unfortunately , the...\n",
       "987                    we get the comedy we settle for\n",
       "988  enchanted with low life tragedy and liberally ...\n",
       "989  ice age wo n't drop your jaw , but it will war...\n",
       "990  like a precious and finely cut diamond , magni...\n",
       "991  the film has a laundry list of minor shortcomi...\n",
       "992  whether it 's the worst movie of 2002 , i ca n...\n",
       "993  exploring value choices is a worthwhile topic ...\n",
       "994  talkiness is n't necessarily bad , but the dia...\n",
       "995  the best thing i can say about this film is th...\n",
       "996  wonder of wonders a teen movie with a humanist...\n",
       "997  the wild thornberrys movie is pleasant enough ...\n",
       "998  as a thoughtful and unflinching examination of...\n",
       "999   i 'd rather watch a rerun of the powerpuff girls\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([predicted_data, test_data], axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+</td>\n",
       "      <td>a seriocomic debut of extravagant promise by g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>while you have to admit it 's semi amusing to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>a boldly stroked , luridly coloured , uni dime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+</td>\n",
       "      <td>an enormously entertaining movie , like nothin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>in his latest effort , storytelling , solondz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>+</td>\n",
       "      <td>missteps take what was otherwise a fascinating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>+</td>\n",
       "      <td>its use of the thriller form to examine the la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-</td>\n",
       "      <td>the master of disguise is awful it 's pauly sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-</td>\n",
       "      <td>when you find yourself rooting for the monster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>+</td>\n",
       "      <td>what 's most memorable about circuit is that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>as saccharine as it is disposable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-</td>\n",
       "      <td>cherish would 've worked a lot better had it b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>+</td>\n",
       "      <td>with ichi the killer , takashi miike , japan '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>the filmmakers are playing to the big boys in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>+</td>\n",
       "      <td>this latest installment of the horror film fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-</td>\n",
       "      <td>smith finds amusing juxtapositions that justif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-</td>\n",
       "      <td>those eternally devoted to the insanity of bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-</td>\n",
       "      <td>a bland , obnoxious 88 minute infomercial for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>+</td>\n",
       "      <td>this is a movie that refreshes the mind and sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-</td>\n",
       "      <td>eckstraordinarily lame and severely boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>+</td>\n",
       "      <td>both exuberantly romantic and serenely melanch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-</td>\n",
       "      <td>the movie is n't painfully bad , something to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>+</td>\n",
       "      <td>rather than real figures , elling and kjell bj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>+</td>\n",
       "      <td>though it was made with careful attention to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-</td>\n",
       "      <td>all the queen 's men is a throwback war movie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-</td>\n",
       "      <td>an adorably whimsical comedy that deserves mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>+</td>\n",
       "      <td>this submarine drama earns the right to be fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>+</td>\n",
       "      <td>a compelling story of musical passion against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-</td>\n",
       "      <td>it has the right approach and the right openin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-</td>\n",
       "      <td>according to wendigo , ` nature ' loves the me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>-</td>\n",
       "      <td>it 's a shame that the storyline and its under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>-</td>\n",
       "      <td>if anything , see it for karen black , who cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>+</td>\n",
       "      <td>an intimate contemplation of two marvelously m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>-</td>\n",
       "      <td>the only type of lives this glossy comedy dram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>-</td>\n",
       "      <td>proves a lovely trifle that , unfortunately , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>+</td>\n",
       "      <td>enormously enjoyable , high adrenaline documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>+</td>\n",
       "      <td>oddly , the film is n't nearly as downbeat as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>-</td>\n",
       "      <td>an unsophisticated sci fi drama that takes its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>-</td>\n",
       "      <td>once the expectation of laughter has been quas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>-</td>\n",
       "      <td>one of the worst movies of the year watching i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>+</td>\n",
       "      <td>manages to delight without much of a story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>-</td>\n",
       "      <td>although it starts off so bad that you feel li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>+</td>\n",
       "      <td>maryam is a small film , but it offers large r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>+</td>\n",
       "      <td>poignant if familiar story of a young person s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>+</td>\n",
       "      <td>a ho hum affair , always watchable yet hardly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>-</td>\n",
       "      <td>the gags that fly at such a furiously funny pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>-</td>\n",
       "      <td>it 's not a bad plot but , unfortunately , the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>-</td>\n",
       "      <td>we get the comedy we settle for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>+</td>\n",
       "      <td>enchanted with low life tragedy and liberally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>+</td>\n",
       "      <td>ice age wo n't drop your jaw , but it will war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>+</td>\n",
       "      <td>like a precious and finely cut diamond , magni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>+</td>\n",
       "      <td>the film has a laundry list of minor shortcomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>-</td>\n",
       "      <td>whether it 's the worst movie of 2002 , i ca n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>+</td>\n",
       "      <td>exploring value choices is a worthwhile topic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>-</td>\n",
       "      <td>talkiness is n't necessarily bad , but the dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-</td>\n",
       "      <td>the best thing i can say about this film is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>+</td>\n",
       "      <td>wonder of wonders a teen movie with a humanist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>+</td>\n",
       "      <td>the wild thornberrys movie is pleasant enough ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>+</td>\n",
       "      <td>as a thoughtful and unflinching examination of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-</td>\n",
       "      <td>i 'd rather watch a rerun of the powerpuff girls</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                                                  1\n",
       "0    +  a seriocomic debut of extravagant promise by g...\n",
       "1    -  while you have to admit it 's semi amusing to ...\n",
       "2    -  a boldly stroked , luridly coloured , uni dime...\n",
       "3    +  an enormously entertaining movie , like nothin...\n",
       "4    -  in his latest effort , storytelling , solondz ...\n",
       "5    +  missteps take what was otherwise a fascinating...\n",
       "6    +  its use of the thriller form to examine the la...\n",
       "7    -  the master of disguise is awful it 's pauly sh...\n",
       "8    -  when you find yourself rooting for the monster...\n",
       "9    +  what 's most memorable about circuit is that i...\n",
       "10   -                  as saccharine as it is disposable\n",
       "11   -  cherish would 've worked a lot better had it b...\n",
       "12   +  with ichi the killer , takashi miike , japan '...\n",
       "13   -  the filmmakers are playing to the big boys in ...\n",
       "14   +  this latest installment of the horror film fra...\n",
       "15   -  smith finds amusing juxtapositions that justif...\n",
       "16   -  those eternally devoted to the insanity of bla...\n",
       "17   -  a bland , obnoxious 88 minute infomercial for ...\n",
       "18   +  this is a movie that refreshes the mind and sp...\n",
       "19   -         eckstraordinarily lame and severely boring\n",
       "20   +  both exuberantly romantic and serenely melanch...\n",
       "21   -  the movie is n't painfully bad , something to ...\n",
       "22   +  rather than real figures , elling and kjell bj...\n",
       "23   +  though it was made with careful attention to d...\n",
       "24   -  all the queen 's men is a throwback war movie ...\n",
       "25   -  an adorably whimsical comedy that deserves mor...\n",
       "26   +  this submarine drama earns the right to be fav...\n",
       "27   +  a compelling story of musical passion against ...\n",
       "28   -  it has the right approach and the right openin...\n",
       "29   -  according to wendigo , ` nature ' loves the me...\n",
       "..  ..                                                ...\n",
       "970  -  it 's a shame that the storyline and its under...\n",
       "971  -  if anything , see it for karen black , who cam...\n",
       "972  +  an intimate contemplation of two marvelously m...\n",
       "973  -  the only type of lives this glossy comedy dram...\n",
       "974  -  proves a lovely trifle that , unfortunately , ...\n",
       "975  +  enormously enjoyable , high adrenaline documen...\n",
       "976  +  oddly , the film is n't nearly as downbeat as ...\n",
       "977  -  an unsophisticated sci fi drama that takes its...\n",
       "978  -  once the expectation of laughter has been quas...\n",
       "979  -  one of the worst movies of the year watching i...\n",
       "980  +         manages to delight without much of a story\n",
       "981  -  although it starts off so bad that you feel li...\n",
       "982  +  maryam is a small film , but it offers large r...\n",
       "983  +  poignant if familiar story of a young person s...\n",
       "984  +  a ho hum affair , always watchable yet hardly ...\n",
       "985  -  the gags that fly at such a furiously funny pa...\n",
       "986  -  it 's not a bad plot but , unfortunately , the...\n",
       "987  -                    we get the comedy we settle for\n",
       "988  +  enchanted with low life tragedy and liberally ...\n",
       "989  +  ice age wo n't drop your jaw , but it will war...\n",
       "990  +  like a precious and finely cut diamond , magni...\n",
       "991  +  the film has a laundry list of minor shortcomi...\n",
       "992  -  whether it 's the worst movie of 2002 , i ca n...\n",
       "993  +  exploring value choices is a worthwhile topic ...\n",
       "994  -  talkiness is n't necessarily bad , but the dia...\n",
       "995  -  the best thing i can say about this film is th...\n",
       "996  +  wonder of wonders a teen movie with a humanist...\n",
       "997  +  the wild thornberrys movie is pleasant enough ...\n",
       "998  +  as a thoughtful and unflinching examination of...\n",
       "999  -   i 'd rather watch a rerun of the powerpuff girls\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test.txt.predicted.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
